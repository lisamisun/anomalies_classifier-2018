{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting raw data from *_dump.tx files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from uuid import UUID\n",
    "from ast import literal_eval\n",
    "\n",
    "from parse_tree_node import ParseTreeNode      \n",
    "\n",
    "with open(\"datasets/xss_dump.tx\", 'r') as xss:\n",
    "    raw_xss_train = eval(xss.read())\n",
    "    xss_trees_train = []\n",
    "    for entry in raw_xss_train:\n",
    "        xss_trees_train.append(ParseTreeNode.from_dict(entry['req_tree']['tree']))\n",
    "\n",
    "with open(\"datasets/sqli_dump.tx\", 'r') as sqli:\n",
    "    raw_sqli_train = eval(sqli.read())\n",
    "    sqli_trees_train = []\n",
    "    for entry in raw_sqli_train:\n",
    "        sqli_trees_train.append(ParseTreeNode.from_dict(entry['req_tree']['tree']))\n",
    "\n",
    "with open(\"datasets/cominj_dump.tx\", 'r') as cominj:\n",
    "    raw_cominj_train = eval(cominj.read())\n",
    "    cominj_trees_train = []\n",
    "    for entry in raw_cominj_train:\n",
    "        cominj_trees_train.append(ParseTreeNode.from_dict(entry['req_tree']['tree']))\n",
    "        \n",
    "with open(\"test_datasets/xss_dump.tx\", 'r') as xss:\n",
    "    raw_xss_test = eval(xss.read())\n",
    "    xss_trees_test = []\n",
    "    for entry in raw_xss_test:\n",
    "        xss_trees_test.append(ParseTreeNode.from_dict(entry['req_tree']['tree']))\n",
    "        \n",
    "with open(\"test_datasets/sqli_dump.tx\", 'r') as sqli:\n",
    "    raw_sqli_test = eval(sqli.read())\n",
    "    sqli_trees_test = []\n",
    "    for entry in raw_sqli_test:\n",
    "        sqli_trees_test.append(ParseTreeNode.from_dict(entry['req_tree']['tree']))\n",
    "        \n",
    "with open(\"test_datasets/cominj_dump.tx\", 'r') as cominj:\n",
    "    raw_cominj_test = eval(cominj.read())\n",
    "    cominj_trees_test = []\n",
    "    for entry in raw_cominj_test:\n",
    "        cominj_trees_test.append(ParseTreeNode.from_dict(entry['req_tree']['tree']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepocessing request trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "275\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from urllib import unquote_plus\n",
    "\n",
    "# size of a special class\n",
    "CLASS_SIZE_TRAIN = 1124\n",
    "XSS_CLASS_SIZE_TEST = len(xss_trees_test)\n",
    "SQLI_CLASS_SIZE_TEST = len(sqli_trees_test)\n",
    "COMINJ_CLASS_SIZE_TEST = len(cominj_trees_test)\n",
    "\n",
    "# checking the path begins with a subpath from the list_of_subpaths\n",
    "def contains_subpath(path, list_of_subpaths):\n",
    "    for subpath in list_of_subpaths:\n",
    "        if path[:len(subpath)] == subpath:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# extracting values from fields specified by useful_paths\n",
    "def extracting_useful_fields(tree, useful_paths):\n",
    "    feature_string = \"\"\n",
    "    for path, node in tree.walk():\n",
    "        if node.is_leaf() and contains_subpath(path, useful_paths):\n",
    "            feature_string += node.value + ' '\n",
    "    return feature_string[:-1]\n",
    "\n",
    "# building a corpus of useful strings from trees\n",
    "def building_corpus(trees, useful_paths):\n",
    "    corpus = [extracting_useful_fields(entry, useful_paths) for entry in trees]\n",
    "    return corpus\n",
    "   \n",
    "all_trees_train = xss_trees_train + sqli_trees_train + cominj_trees_train\n",
    "all_trees_test = xss_trees_test + sqli_trees_test + cominj_trees_test\n",
    "    \n",
    "# extracting all potentially useful fields from trees    \n",
    "useful_paths = [['url', 'query'], ['headers', 'user-agent'], ['headers', 'referer'], ['headers', 'cookie'], ['body']]\n",
    "full_corpus_train = building_corpus(all_trees_train, useful_paths)\n",
    "full_corpus_test = building_corpus(all_trees_test, useful_paths)\n",
    "\n",
    "# extracting ['url', 'query'] fields from trees\n",
    "url_query_corpus_train = building_corpus(all_trees_train, [['url', 'query']])\n",
    "url_query_corpus_test = building_corpus(all_trees_test, [['url', 'query']])\n",
    "\n",
    "# extracting ['headers', 'user-agent'] fields from trees\n",
    "user_agent_corpus_train = building_corpus(all_trees_train, [['headers', 'user-agent']])\n",
    "user_agent_corpus_test = building_corpus(all_trees_test, [['headers', 'user-agent']])\n",
    "\n",
    "# extracting ['headers', 'referer'] fields from trees\n",
    "referer_corpus_train = building_corpus(all_trees_train, [['headers', 'referer']])\n",
    "referer_corpus_test = building_corpus(all_trees_test, [['headers', 'referer']])\n",
    "\n",
    "# extracting ['headers', 'cookie'] fields from trees\n",
    "cookie_corpus_train = building_corpus(all_trees_train, [['headers', 'cookie']])\n",
    "cookie_corpus_test = building_corpus(all_trees_test, [['headers', 'cookie']])\n",
    "\n",
    "# extracting ['body'] fields from trees\n",
    "body_corpus_train = building_corpus(all_trees_train, [['body']])\n",
    "body_corpus_test = building_corpus(all_trees_test, [['body']])\n",
    "\n",
    "# class labels list\n",
    "labels_corpus_train = [0 for i in range(CLASS_SIZE_TRAIN)] + [1 for i in range(CLASS_SIZE_TRAIN)] + [2 for i in range(CLASS_SIZE_TRAIN)]\n",
    "labels_corpus_test = [0 for i in range(XSS_CLASS_SIZE_TEST)] + [1 for i in range(SQLI_CLASS_SIZE_TEST)] + [2 for i in range(COMINJ_CLASS_SIZE_TEST)]\n",
    "labels_names = [\"XSS\", \"SQL Injection\", \"Command Injection\"]\n",
    "\n",
    "# mixing entries in corpuses\n",
    "zip_corpus_train = zip(full_corpus_train, url_query_corpus_train, user_agent_corpus_train, referer_corpus_train, \n",
    "                       cookie_corpus_train, body_corpus_train, labels_corpus_train)\n",
    "shuffle(zip_corpus_train)\n",
    "full_corpus_train, url_query_corpus_train, user_agent_corpus_train, referer_corpus_train, cookie_corpus_train, body_corpus_train, labels_corpus_train = zip(*zip_corpus_train)\n",
    "\n",
    "zip_corpus_test = zip(full_corpus_test, url_query_corpus_test, user_agent_corpus_test, referer_corpus_test, \n",
    "                       cookie_corpus_test, body_corpus_test, labels_corpus_test)\n",
    "shuffle(zip_corpus_test)\n",
    "full_corpus_test, url_query_corpus_test, user_agent_corpus_test, referer_corpus_test, cookie_corpus_test, body_corpus_test, labels_corpus_test = zip(*zip_corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features from train corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# crawler specific words - delete from resulting tokens\n",
    "stop_words = ['gq435t372gnkq2inkmh2j5p444', '3fi7p4bkf6ug0qtrd2erlon453', 'cominj', 'sqli', 'xss']\n",
    "\n",
    "# preprocessing includes lowercasing and decoding urlencode for feature_string - fix this comment\n",
    "def preprocessing_feature_string(feature_string):\n",
    "    feature_string = feature_string.lower()\n",
    "    while True:\n",
    "        unquote_feature_string = unquote_plus(feature_string)\n",
    "        if unquote_feature_string != feature_string:\n",
    "            feature_string = unquote_feature_string\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    # replacing all numbers by 'NUM'\n",
    "    # feature_string = re.sub(r'(?u)\\b\\d+\\b', u'NUM', feature_string)\n",
    "    \n",
    "    feature_string = re.sub(r'(?u)sqlmap/1\\.2\\.4\\.10#dev \\(http://sqlmap\\.org\\)', u' ', feature_string)\n",
    "    feature_string = re.sub(r'(?u)commix/v2\\.4-dev#27 \\(http://commixproject\\.com\\)', u' ', feature_string)\n",
    "    feature_string = re.sub(r'(?u)http://10\\.0\\.0\\.2:80/bwapp/xss_get\\.php', u' ', feature_string)\n",
    "    feature_string = re.sub(r'(?u)http://10\\.0\\.0\\.2:80/bwapp/xss_post\\.php', u' ', feature_string)\n",
    "    \n",
    "    feature_string = re.sub(r'(?u)\\b\\d+\\b', u' ', feature_string)\n",
    "    \n",
    "    feature_string = re.sub(r'(?u)http://www\\.w3\\.org/ /xml-events', u' ', feature_string)\n",
    "    feature_string = re.sub(r'(?u)http://www\\.w3\\.org/ /svg', u' ', feature_string)\n",
    "    \n",
    "    # print feature_string\n",
    "    \n",
    "    return feature_string\n",
    "\n",
    "# maybe useful: analyzer - char n-grams from raw string\n",
    "# extracting features from full_corpus: words...\n",
    "vectorizer_words = CountVectorizer(decode_error='ignore', preprocessor=preprocessing_feature_string, ngram_range=(1,2), stop_words=stop_words, token_pattern=u'(?u)\\\\b\\\\w+\\\\b', binary=True)\n",
    "full_words_matrix_train = vectorizer_words.fit_transform(full_corpus_train)\n",
    "# ...and nonalphabetic simbols\n",
    "vectorizer_nonalphabetic = CountVectorizer(decode_error='ignore', preprocessor=preprocessing_feature_string, token_pattern=u'(?u)[^\\\\w\\\\s]', binary=True)\n",
    "full_nonalphabetic_matrix_train = vectorizer_nonalphabetic.fit_transform(full_corpus_train)\n",
    "\n",
    "# extracting features from url_query_corpus - corpus that contains only parameter values from the url:query fields\n",
    "vectorizer_url_query = CountVectorizer(decode_error='ignore', preprocessor=preprocessing_feature_string, stop_words=stop_words, token_pattern=u'(?u)\\\\b\\\\w+\\\\b', binary=True)\n",
    "url_query_matrix_train = vectorizer_url_query.fit_transform(url_query_corpus_train)\n",
    "\n",
    "# extracting features from user_agent_corpus - corpus that contains only parameter values from the headers:user-agent fields\n",
    "vectorizer_user_agent = CountVectorizer(decode_error='ignore', preprocessor=preprocessing_feature_string, stop_words=stop_words, token_pattern=u'(?u)\\\\b\\\\w+\\\\b', binary=True)\n",
    "user_agent_matrix_train = vectorizer_user_agent.fit_transform(user_agent_corpus_train)\n",
    "\n",
    "# extracting features from referer_corpus - corpus that contains only parameter values from the headers:referer fields\n",
    "vectorizer_referer = CountVectorizer(decode_error='ignore', preprocessor=preprocessing_feature_string, stop_words=stop_words, token_pattern=u'(?u)\\\\b\\\\w+\\\\b', binary=True)\n",
    "referer_matrix_train = vectorizer_referer.fit_transform(referer_corpus_train)\n",
    "\n",
    "# extracting features from cookie_corpus - corpus that contains only parameter values from the headers:cookie fields\n",
    "vectorizer_cookie = CountVectorizer(decode_error='ignore', preprocessor=preprocessing_feature_string, stop_words=stop_words, token_pattern=u'(?u)\\\\b\\\\w+\\\\b', binary=True)\n",
    "cookie_matrix_train = vectorizer_cookie.fit_transform(cookie_corpus_train)\n",
    "\n",
    "# extracting features from body_corpus - corpus that contains only parameter values from the body fields\n",
    "vectorizer_body = CountVectorizer(decode_error='ignore', preprocessor=preprocessing_feature_string, stop_words=stop_words, token_pattern=u'(?u)\\\\b\\\\w+\\\\b', binary=True)\n",
    "body_matrix_train = vectorizer_body.fit_transform(body_corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features from test corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features from full_corpus: words...\n",
    "full_words_matrix_test = vectorizer_words.transform(full_corpus_test)\n",
    "# ...and nonalphabetic simbols\n",
    "full_nonalphabetic_matrix_test = vectorizer_nonalphabetic.transform(full_corpus_test)\n",
    "\n",
    "# extracting features from url_query_corpus - corpus that contains only parameter values from the url:query fields\n",
    "url_query_matrix_test = vectorizer_url_query.transform(url_query_corpus_test)\n",
    "\n",
    "# extracting features from user_agent_corpus - corpus that contains only parameter values from the headers:user-agent fields\n",
    "user_agent_matrix_test = vectorizer_user_agent.transform(user_agent_corpus_test)\n",
    "\n",
    "# extracting features from referer_corpus - corpus that contains only parameter values from the headers:referer fields\n",
    "referer_matrix_test = vectorizer_referer.transform(referer_corpus_test)\n",
    "\n",
    "# extracting features from cookie_corpus - corpus that contains only parameter values from the headers:cookie fields\n",
    "cookie_matrix_test = vectorizer_cookie.transform(cookie_corpus_test)\n",
    "\n",
    "# extracting features from body_corpus - corpus that contains only parameter values from the body fields\n",
    "body_matrix_test = vectorizer_body.transform(body_corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the feature matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' features = get_all_feature_names()\\nfor feature in features:\\n    print feature '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "def get_all_feature_names():\n",
    "    all_feature_names = []\n",
    "    all_feature_names += vectorizer_words.get_feature_names() + vectorizer_nonalphabetic.get_feature_names()\n",
    "    all_feature_names += ['url:query:' + feature_name for feature_name in vectorizer_url_query.get_feature_names()]\n",
    "    all_feature_names += ['headers:user-agent:' + feature_name for feature_name in vectorizer_user_agent.get_feature_names()]\n",
    "    all_feature_names += ['headers:referer:' + feature_name for feature_name in vectorizer_referer.get_feature_names()]\n",
    "    all_feature_names += ['headers:cookie:' + feature_name for feature_name in vectorizer_cookie.get_feature_names()]\n",
    "    all_feature_names += ['body:' + feature_name for feature_name in vectorizer_body.get_feature_names()]\n",
    "    return all_feature_names\n",
    "\n",
    "features_matrix_train = hstack([full_words_matrix_train, full_nonalphabetic_matrix_train, url_query_matrix_train, \n",
    "                                user_agent_matrix_train, referer_matrix_train, cookie_matrix_train, body_matrix_train], format='csr')\n",
    "\n",
    "features_matrix_test = hstack([full_words_matrix_test, full_nonalphabetic_matrix_test, url_query_matrix_test, \n",
    "                                user_agent_matrix_test, referer_matrix_test, cookie_matrix_test, body_matrix_test], format='csr')\n",
    "'''\n",
    "\n",
    "features_matrix_train = hstack([full_words_matrix_train, url_query_matrix_train, \n",
    "                                user_agent_matrix_train, referer_matrix_train, cookie_matrix_train, body_matrix_train], format='csr')\n",
    "\n",
    "features_matrix_test = hstack([full_words_matrix_test, url_query_matrix_test, \n",
    "                                user_agent_matrix_test, referer_matrix_test, cookie_matrix_test, body_matrix_test], format='csr')\n",
    "''' \n",
    "\n",
    "# labels column in Compressed Sparse Column format\n",
    "labels_train = csr_matrix(list(labels_corpus_train)).T\n",
    "labels_test = csr_matrix(list(labels_corpus_test)).T\n",
    "\n",
    "# printing all feature names\n",
    "''' features = get_all_feature_names()\n",
    "for feature in features:\n",
    "    print feature '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes:\n",
      "ratio: 83:506\n",
      "score: 0.16403162055335968\n",
      "\n",
      "XSS: headers:referer:script | ; | ( | ) | url:query:javascript | headers:cookie:src | headers:referer:src | headers:user-agent:src | body:javascript | src | headers:user-agent:javascript | headers:referer:javascript | headers:cookie:javascript | javascript | : | \" | / | = | < | >\n",
      "SQL Injection: headers:cookie:script | ( | url:query:javascript | ) | script | - | src | headers:referer:src | headers:user-agent:src | ; | headers:cookie:javascript | headers:user-agent:javascript | headers:referer:javascript | javascript | : | \" | / | = | > | <\n",
      "Command Injection: headers:cookie:script | headers:user-agent:style | headers:referer:style | style | headers:cookie:style | url:query:javascript | - | ; | javascript | headers:cookie:javascript | headers:referer:javascript | headers:user-agent:javascript | ( | ) | \" | : | / | = | > | <\n",
      "\n",
      "Logistic Regression (liblinear):\n",
      "ratio: 72:506\n",
      "score: 0.1422924901185771\n",
      "\n",
      "XSS: svg script | 8de9f5ee95a86e66effec4da36a7e475 | url:query:9ee7778fdddc30340ef65ccdac817415 | 8de9f5ee95a86e66effec4da36a7e475 meta | headers:referer:8de9f5ee95a86e66effec4da36a7e475 | headers:cookie:2289516469b37a22c76ab3f48b9a3ccf | url 9ee7778fdddc30340ef65ccdac817415 | f7c2fc8f8143d38e54587a9e45ea3034 | headers:user-agent:f7c2fc8f8143d38e54587a9e45ea3034 | url f7c2fc8f8143d38e54587a9e45ea3034 | url 8de9f5ee95a86e66effec4da36a7e475 | url 2289516469b37a22c76ab3f48b9a3ccf | 9ee7778fdddc30340ef65ccdac817415 | 2289516469b37a22c76ab3f48b9a3ccf meta | 2289516469b37a22c76ab3f48b9a3ccf | f7c2fc8f8143d38e54587a9e45ea3034 meta | onmousemove javascript | javascript javascript | style type | body:javascript\n",
      "SQL Injection: headers:user-agent:autofocus | headers:referer:autofocus | url:query:href | onload svg | headers:referer:onmouseover | headers:user-agent:onmouseover | headers:cookie:onmouseover | onmouseover | onload | headers:cookie:onload | headers:user-agent:onload | headers:referer:onload | / | url:query:html | onload javascript | % | ' | # | svg onload | ;\n",
      "Command Injection: headers:referer:style | onerror javascript | url:query:javascript | url:query:style | headers:cookie:style | iframe src | headers:user-agent:xml | xml | headers:referer:xml | : | headers:cookie:xml | , | alert | headers:cookie:alert | headers:referer:alert | headers:user-agent:alert | ) | ( | url:query:alert | .\n",
      "\n",
      "Logistic Regression (lbfgs):\n",
      "ratio: 182:506\n",
      "score: 0.35968379446640314\n",
      "\n",
      "XSS: javascript javascript | onmousemove javascript | url 8de9f5ee95a86e66effec4da36a7e475 | headers:referer:8de9f5ee95a86e66effec4da36a7e475 | 2289516469b37a22c76ab3f48b9a3ccf | 2289516469b37a22c76ab3f48b9a3ccf meta | headers:cookie:2289516469b37a22c76ab3f48b9a3ccf | url:query:9ee7778fdddc30340ef65ccdac817415 | headers:user-agent:f7c2fc8f8143d38e54587a9e45ea3034 | 8de9f5ee95a86e66effec4da36a7e475 | 8de9f5ee95a86e66effec4da36a7e475 meta | f7c2fc8f8143d38e54587a9e45ea3034 | 9ee7778fdddc30340ef65ccdac817415 | url f7c2fc8f8143d38e54587a9e45ea3034 | url 9ee7778fdddc30340ef65ccdac817415 | url 2289516469b37a22c76ab3f48b9a3ccf | f7c2fc8f8143d38e54587a9e45ea3034 meta | svg script | body:javascript | >\n",
      "SQL Injection: headers:cookie:autofocus | headers:user-agent:autofocus | url:query:href | onload svg | headers:user-agent:onmouseover | onmouseover | headers:cookie:onmouseover | headers:referer:onmouseover | / | headers:cookie:onload | onload | headers:referer:onload | headers:user-agent:onload | url:query:html | onload javascript | % | ' | # | svg onload | ;\n",
      "Command Injection: headers:user-agent:style | headers:referer:style | style | url:query:javascript | headers:cookie:style | xml | headers:referer:xml | headers:user-agent:xml | iframe src | headers:cookie:xml | , | alert | headers:referer:alert | headers:cookie:alert | headers:user-agent:alert | ) | url:query:alert | : | ( | .\n",
      "\n",
      "Logistic Regression (newton-cg):\n",
      "ratio: 182:506\n",
      "score: 0.35968379446640314\n",
      "\n",
      "XSS: javascript javascript | onmousemove javascript | url 8de9f5ee95a86e66effec4da36a7e475 | headers:referer:8de9f5ee95a86e66effec4da36a7e475 | 2289516469b37a22c76ab3f48b9a3ccf | 2289516469b37a22c76ab3f48b9a3ccf meta | headers:cookie:2289516469b37a22c76ab3f48b9a3ccf | url:query:9ee7778fdddc30340ef65ccdac817415 | headers:user-agent:f7c2fc8f8143d38e54587a9e45ea3034 | 8de9f5ee95a86e66effec4da36a7e475 | 8de9f5ee95a86e66effec4da36a7e475 meta | f7c2fc8f8143d38e54587a9e45ea3034 | 9ee7778fdddc30340ef65ccdac817415 | url f7c2fc8f8143d38e54587a9e45ea3034 | url 9ee7778fdddc30340ef65ccdac817415 | url 2289516469b37a22c76ab3f48b9a3ccf | f7c2fc8f8143d38e54587a9e45ea3034 meta | svg script | body:javascript | >\n",
      "SQL Injection: headers:cookie:autofocus | headers:user-agent:autofocus | url:query:href | onload svg | headers:user-agent:onmouseover | onmouseover | headers:cookie:onmouseover | headers:referer:onmouseover | / | headers:cookie:onload | onload | headers:referer:onload | headers:user-agent:onload | url:query:html | onload javascript | % | ' | # | svg onload | ;\n",
      "Command Injection: headers:user-agent:style | headers:referer:style | style | url:query:javascript | headers:cookie:style | xml | headers:referer:xml | headers:user-agent:xml | iframe src | headers:cookie:xml | , | alert | headers:referer:alert | headers:cookie:alert | headers:user-agent:alert | ) | url:query:alert | : | ( | .\n",
      "\n",
      "Logistic Regression (sag):\n",
      "ratio: 73:506\n",
      "score: 0.1442687747035573\n",
      "\n",
      "XSS: style type | svg script | onmousemove javascript | 2289516469b37a22c76ab3f48b9a3ccf | url:query:9ee7778fdddc30340ef65ccdac817415 | url 2289516469b37a22c76ab3f48b9a3ccf | f7c2fc8f8143d38e54587a9e45ea3034 meta | f7c2fc8f8143d38e54587a9e45ea3034 | url 8de9f5ee95a86e66effec4da36a7e475 | url 9ee7778fdddc30340ef65ccdac817415 | headers:user-agent:f7c2fc8f8143d38e54587a9e45ea3034 | url f7c2fc8f8143d38e54587a9e45ea3034 | headers:cookie:2289516469b37a22c76ab3f48b9a3ccf | 8de9f5ee95a86e66effec4da36a7e475 | 9ee7778fdddc30340ef65ccdac817415 | 8de9f5ee95a86e66effec4da36a7e475 meta | 2289516469b37a22c76ab3f48b9a3ccf meta | headers:referer:8de9f5ee95a86e66effec4da36a7e475 | body:javascript | >\n",
      "SQL Injection: headers:cookie:onmouseover | headers:referer:onmouseover | headers:user-agent:onmouseover | \" | headers:user-agent:autofocus | headers:cookie:autofocus | autofocus | headers:referer:autofocus | headers:cookie:onload | headers:user-agent:onload | onload | headers:referer:onload | url:query:html | onload javascript | % | # | ' | / | svg onload | ;\n",
      "Command Injection: headers:user-agent:style | style | headers:referer:style | url:query:javascript | headers:cookie:style | iframe src | headers:user-agent:xml | headers:referer:xml | xml | headers:cookie:xml | , | headers:cookie:alert | alert | headers:user-agent:alert | headers:referer:alert | : | ) | url:query:alert | ( | .\n",
      "\n",
      "Logistic Regression (saga):\n",
      "ratio: 72:506\n",
      "score: 0.1422924901185771\n",
      "\n",
      "XSS: javascript javascript | svg script | url 2289516469b37a22c76ab3f48b9a3ccf | url 9ee7778fdddc30340ef65ccdac817415 | url f7c2fc8f8143d38e54587a9e45ea3034 | url 8de9f5ee95a86e66effec4da36a7e475 | headers:cookie:2289516469b37a22c76ab3f48b9a3ccf | url:query:9ee7778fdddc30340ef65ccdac817415 | 2289516469b37a22c76ab3f48b9a3ccf | 2289516469b37a22c76ab3f48b9a3ccf meta | f7c2fc8f8143d38e54587a9e45ea3034 | 8de9f5ee95a86e66effec4da36a7e475 | 8de9f5ee95a86e66effec4da36a7e475 meta | 9ee7778fdddc30340ef65ccdac817415 | f7c2fc8f8143d38e54587a9e45ea3034 meta | headers:referer:8de9f5ee95a86e66effec4da36a7e475 | headers:user-agent:f7c2fc8f8143d38e54587a9e45ea3034 | onmousemove javascript | body:javascript | >\n",
      "SQL Injection: headers:user-agent:onmouseover | headers:referer:onmouseover | headers:cookie:onmouseover | \" | headers:user-agent:autofocus | autofocus | headers:cookie:autofocus | headers:referer:autofocus | headers:user-agent:onload | onload | headers:referer:onload | headers:cookie:onload | url:query:html | onload javascript | % | # | ' | / | svg onload | ;\n",
      "Command Injection: headers:referer:style | headers:user-agent:style | style | url:query:javascript | headers:cookie:style | iframe src | xml | headers:user-agent:xml | headers:referer:xml | headers:cookie:xml | , | headers:cookie:alert | alert | headers:referer:alert | headers:user-agent:alert | : | ) | url:query:alert | ( | .\n",
      "\n",
      "Linear SVC (ovr):\n",
      "ratio: 81:506\n",
      "score: 0.1600790513833992\n",
      "\n",
      "XSS: 2e9c9c1dcf65f7b0c13df679119a2bce | a2a7f1c2603df1e245449094df44a7fc ca022cc1333145a098eeb14b6b591183 | headers:referer:2e9c9c1dcf65f7b0c13df679119a2bce | body:javascript | url 2289516469b37a22c76ab3f48b9a3ccf | headers:referer:8de9f5ee95a86e66effec4da36a7e475 | 2289516469b37a22c76ab3f48b9a3ccf meta | url f7c2fc8f8143d38e54587a9e45ea3034 | url 9ee7778fdddc30340ef65ccdac817415 | 9ee7778fdddc30340ef65ccdac817415 | url:query:9ee7778fdddc30340ef65ccdac817415 | f7c2fc8f8143d38e54587a9e45ea3034 meta | 8de9f5ee95a86e66effec4da36a7e475 | 2289516469b37a22c76ab3f48b9a3ccf | headers:cookie:2289516469b37a22c76ab3f48b9a3ccf | f7c2fc8f8143d38e54587a9e45ea3034 | headers:user-agent:f7c2fc8f8143d38e54587a9e45ea3034 | 8de9f5ee95a86e66effec4da36a7e475 meta | url 8de9f5ee95a86e66effec4da36a7e475 | >\n",
      "SQL Injection: headers:referer:autofocus | headers:user-agent:autofocus | headers:cookie:autofocus | html onmouseover | onmouseover html | script script | headers:cookie:onmouseover | onmouseover | headers:referer:onmouseover | headers:user-agent:onmouseover | onload svg | url:query:href | url:query:html | onload javascript | / | ' | % | # | svg onload | ;\n",
      "Command Injection: url:query:import | url:query:javascript | onerror javascript | headers:cookie:style | url:query:document | headers:user-agent:xml | headers:referer:xml | xml | ) | iframe src | headers:cookie:xml | headers:referer:alert | headers:cookie:alert | alert | headers:user-agent:alert | ( | , | : | url:query:alert | .\n",
      "\n",
      "Linear SVC (multi_class):\n",
      "ratio: 85:506\n",
      "score: 0.16798418972332016\n",
      "\n",
      "XSS: headers:referer:2e9c9c1dcf65f7b0c13df679119a2bce | headers:cookie:ca022cc1333145a098eeb14b6b591183 | ac232126ad3268a18399edc9811a0bcf a2a7f1c2603df1e245449094df44a7fc | ac232126ad3268a18399edc9811a0bcf | 9ee7778fdddc30340ef65ccdac817415 | url f7c2fc8f8143d38e54587a9e45ea3034 | url 9ee7778fdddc30340ef65ccdac817415 | url 8de9f5ee95a86e66effec4da36a7e475 | 2289516469b37a22c76ab3f48b9a3ccf | headers:referer:8de9f5ee95a86e66effec4da36a7e475 | 2289516469b37a22c76ab3f48b9a3ccf meta | 8de9f5ee95a86e66effec4da36a7e475 meta | url:query:9ee7778fdddc30340ef65ccdac817415 | headers:cookie:2289516469b37a22c76ab3f48b9a3ccf | f7c2fc8f8143d38e54587a9e45ea3034 | f7c2fc8f8143d38e54587a9e45ea3034 meta | headers:user-agent:f7c2fc8f8143d38e54587a9e45ea3034 | url 2289516469b37a22c76ab3f48b9a3ccf | 8de9f5ee95a86e66effec4da36a7e475 | >\n",
      "SQL Injection: d3dca7bb0e7eba33b19ac256921231d5 | 75964620c781fa4bd6a14df7895feffb | 670966aa790711610696fbab88ab4a99 d3dca7bb0e7eba33b19ac256921231d5 | headers:cookie:d3dca7bb0e7eba33b19ac256921231d5 | body:a2a92420a54895033174033afc88c8f6 | headers:user-agent:670966aa790711610696fbab88ab4a99 | a2a92420a54895033174033afc88c8f6 | a2a92420a54895033174033afc88c8f6 670966aa790711610696fbab88ab4a99 | headers:referer:75964620c781fa4bd6a14df7895feffb | d3dca7bb0e7eba33b19ac256921231d5 75964620c781fa4bd6a14df7895feffb | 670966aa790711610696fbab88ab4a99 | url:query:html | \" | onload javascript | ' | / | # | % | svg onload | ;\n",
      "Command Injection: headers:referer:style | headers:cookie:iframe | url:query:javascript | url:query:document | headers:cookie:style | headers:user-agent:xml | headers:referer:xml | xml | ) | iframe src | headers:cookie:xml | headers:referer:alert | alert | headers:cookie:alert | headers:user-agent:alert | , | ( | : | url:query:alert | .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" train_and_test(MLPClassifier(), 'MLPClassifier') \""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import argsort\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# printng features with the highest coefficient values, per class\n",
    "def top20_features(classifier, labels_names):\n",
    "    feature_names = get_all_feature_names()\n",
    "    for i, label_name in enumerate(labels_names):\n",
    "        top20 = argsort(classifier.coef_[i])[-20:]\n",
    "        print(\"%s: %s\" % (label_name, \" | \".join(feature_names[j] for j in top20)))\n",
    "\n",
    "def train_and_test(classifier, classifier_name):\n",
    "    if isinstance(classifier, MLPClassifier):\n",
    "        classifier.fit(features_matrix_train, labels_train.toarray().ravel())\n",
    "    else:\n",
    "        classifier.fit(features_matrix_train, labels_train.toarray())\n",
    "    \n",
    "    labels_prediction = classifier.predict(features_matrix_test)\n",
    "    positive = 0\n",
    "    test_len = len(labels_corpus_test)\n",
    "    for i in range(test_len):\n",
    "        if labels_corpus_test[i] == labels_prediction[i]:\n",
    "            positive += 1\n",
    "    \n",
    "    result = classifier_name + ':\\n'\n",
    "    result +=  'ratio: ' + str(positive) + ':' + str(test_len) + '\\n'\n",
    "    result +=  'score: ' + str(classifier.score(features_matrix_test, labels_test.toarray())) + '\\n'\n",
    "        \n",
    "    print result\n",
    "        \n",
    "    top20_features(classifier, labels_names)\n",
    "        \n",
    "    print\n",
    "        \n",
    "train_and_test(MultinomialNB(), 'Multinomial Naive Bayes')\n",
    "train_and_test(LogisticRegression(dual=True), 'Logistic Regression (liblinear)')\n",
    "train_and_test(LogisticRegression(solver='lbfgs', multi_class='multinomial'), 'Logistic Regression (lbfgs)')\n",
    "train_and_test(LogisticRegression(solver='newton-cg', multi_class='multinomial'), 'Logistic Regression (newton-cg)')\n",
    "train_and_test(LogisticRegression(solver='sag', multi_class='multinomial'), 'Logistic Regression (sag)')\n",
    "train_and_test(LogisticRegression(solver='saga', multi_class='multinomial'), 'Logistic Regression (saga)')\n",
    "train_and_test(LinearSVC(dual=True), 'Linear SVC (ovr)')\n",
    "train_and_test(LinearSVC(multi_class='crammer_singer'), 'Linear SVC (multi_class)')\n",
    "''' train_and_test(MLPClassifier(), 'MLPClassifier') '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
